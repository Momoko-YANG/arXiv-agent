"""
Paper Aggregator â€” èšåˆæ‰€æœ‰æ•°æ®æº + è¯„åˆ† + ç­›é€‰ + æ‘˜è¦

ä¼˜åŒ–åŽçš„æµæ°´çº¿ï¼ˆæ¯” v1 å¿« 3-5xï¼‰ï¼š
    arXiv â†’ S2 æ‰¹é‡æŸ¥ï¼ˆ1æ¬¡è¯·æ±‚ï¼‰â†’ å…³é”®è¯é¢„ç­› or GPTç­›é€‰
    â†’ è¯„åˆ†æŽ’åº â†’ æˆªå– Top N â†’ Crossref åªæŸ¥ Top N â†’ æ‘˜è¦ â†’ å…¥åº“
"""

from datetime import datetime
from typing import List, Dict

from agents.arxiv_agent import ArxivAgent
from agents.semantic_agent import SemanticScholarClient
from agents.crossref_agent import CrossrefClient
from scoring import (
    ScoringPipeline, CitationScorer, AuthorScorer,
    VenueScorer, FreshnessScorer, KeywordScorer,
)
from summarizer.llm_summarizer import PaperSummarizer, extract_key_sentences
from summarizer.prompt_templates import FILTER_SYSTEM, FILTER_PROMPT
from llm_client import LLMClient
from utils.database import ArxivDatabase


class PaperAggregator:
    """è®ºæ–‡èšåˆ + åˆ†æž Agent"""

    def __init__(self, settings):
        self.settings = settings

        # å„æ•°æ®æº
        self.arxiv = ArxivAgent(categories=settings.categories)
        self.s2 = SemanticScholarClient(api_key=settings.s2_api_key)
        self.cr = CrossrefClient(mailto=settings.crossref_mailto)

        # LLMï¼ˆçº¯ HTTP ç›´è¿žï¼Œæ—  SDK ä¾èµ–ï¼‰
        self.llm = LLMClient(
            api_key=settings.openai_api_key,
            model=settings.openai_model,
        )

        # è¯„åˆ†
        self.scorer = ScoringPipeline([
            CitationScorer(weight=30),
            AuthorScorer(weight=20),
            VenueScorer(weight=20),
            FreshnessScorer(weight=15),
            KeywordScorer(keywords=settings.bonus_keywords, weight=15),
        ])

        # ä¸‰æ®µå¼æ‘˜è¦
        self.summarizer = PaperSummarizer(llm_client=self.llm, language="zh")

        # æ•°æ®åº“
        self.db = ArxivDatabase(db_path=settings.db_path)

    # ------------------------------------------------------------------
    # å®Œæ•´æµæ°´çº¿
    # ------------------------------------------------------------------

    def run_pipeline(self) -> Dict:
        s = self.settings
        print(f"\n{'=' * 80}")
        print(f"å¼€å§‹æ™ºèƒ½æŠ“å–ä¸Žåˆ†æž - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'=' * 80}\n")

        # Step 1: arXiv æŠ“å–
        print("ðŸ“¥ Step 1/6: arXiv æŠ“å–...")
        papers = self.arxiv.fetch_recent_papers(days=s.days, max_results=s.max_results)
        if not papers:
            print("âŒ æ²¡æœ‰æŠ“å–åˆ°è®ºæ–‡")
            return {"papers": [], "relevant": [], "summaries": {}}
        print(f"  âœ… å…± {len(papers)} ç¯‡\n")

        # Step 2: ä¼˜å…ˆå°è¯• GPT ç­›é€‰ï¼ˆå¤±è´¥è‡ªåŠ¨é™çº§ï¼‰
        print("ðŸ¤– Step 2/6: è®ºæ–‡ç­›é€‰...")
        if s.research_interests:
            print("  ä½¿ç”¨ GPT æ™ºèƒ½ç­›é€‰...")
            relevant = self._filter_relevant(papers, s.research_interests)
        else:
            print("  æœªè®¾ç½®ç ”ç©¶å…´è¶£ï¼Œä½¿ç”¨å…³é”®è¯é¢„ç­›é€‰")
            relevant = self._keyword_prefilter(papers)
        print(f"  âœ… ç­›é€‰å‡º {len(relevant)} ç¯‡\n")

        # Step 3: Semantic Scholar æ‰¹é‡æŸ¥ï¼ˆä¸€æ¬¡è¯·æ±‚ï¼‰
        if relevant:
            print("ðŸ“¡ Step 3/6: Semantic Scholar æ‰¹é‡è¡¥å……...")
            self.s2.enrich_papers(relevant)
            print()

        # Step 4: è¯„åˆ†æŽ’åº â†’ æˆªå– Top N â†’ åªå¯¹ Top N æŸ¥ Crossref
        if relevant:
            print("ðŸ“Š Step 4/6: è¯„åˆ†æŽ’åº...")
            self.scorer.rank_papers(relevant)
            self._print_ranking(relevant)

        top_papers = relevant[:s.top_n]

        # åªå¯¹æœ€ç»ˆæŽ¨é€çš„è®ºæ–‡æŸ¥ Crossrefï¼ˆçœ 50%+ æ—¶é—´ï¼‰
        if top_papers:
            print(f"ðŸ“– Crossref éªŒè¯ Top {len(top_papers)} ç¯‡...")
            self.cr.enrich_papers(top_papers)
            # Crossref æ•°æ®å›žæ¥åŽé‡æ–°è¯„åˆ†ä¸€æ¬¡
            self.scorer.rank_papers(top_papers)
            print()

        # Step 5: æ‘˜è¦
        summaries = {}
        if top_papers:
            n = len(top_papers)
            # ç­›é€‰é˜¶æ®µå¤±è´¥åŽï¼Œè¿™é‡Œå†ç»™ LLM ä¸€æ¬¡æœºä¼šï¼ˆæ‘˜è¦ä¼˜å…ˆï¼‰
            self.llm.reset_circuit()
            print(f"ðŸ§  Step 5/6: ä¸‰æ®µå¼æ‘˜è¦ä¼˜å…ˆï¼ˆ{n} ç¯‡ï¼‰")
            summaries = self.summarizer.summarize_batch(top_papers, delay=0.5)
            if not summaries:
                print(f"  âš ï¸  LLM æ‘˜è¦å…¨éƒ¨å¤±è´¥ï¼Œé™çº§è§„åˆ™æ‘˜è¦")
                summaries = self._fallback_summaries(top_papers)
            print(f"  âœ… ç”Ÿæˆ {len(summaries)}/{n} ç¯‡æ‘˜è¦\n")

        # Step 6: å…¥åº“
        print("ðŸ’¾ Step 6/6: ä¿å­˜åˆ°æ•°æ®åº“...")
        new_count = sum(1 for p in papers if self.db.insert_paper(p))
        print(f"  âœ… æ–°å¢ž {new_count} ç¯‡\n")

        return {
            "papers": papers,
            "relevant": top_papers,
            "summaries": summaries,
        }

    # ------------------------------------------------------------------
    # ç­›é€‰æ–¹æ³•
    # ------------------------------------------------------------------

    def _filter_relevant(self, papers: List[Dict],
                         research_interests: str,
                         top_k: int = 10) -> List[Dict]:
        """GPT æ™ºèƒ½ç­›é€‰"""
        papers_text = ""
        for i, p in enumerate(papers):
            papers_text += (
                f"{i+1}. ID: {p['arxiv_id']}\n"
                f"   æ ‡é¢˜: {p['title']}\n"
                f"   æ‘˜è¦: {p['summary'][:300]}...\n\n"
            )

        prompt = FILTER_PROMPT.format(
            research_interests=research_interests,
            papers_text=papers_text,
            top_k=top_k,
        )

        try:
            response = self.llm.generate(prompt, system=FILTER_SYSTEM)
            print("  ðŸ§ª [DEBUG] GPT åŽŸå§‹ response:")
            print(response[:1500] + ("..." if len(response) > 1500 else ""))
            relevant_ids = []
            for line in response.strip().split("\n"):
                line = line.strip()
                if len(line) >= 10 and "." in line:
                    relevant_ids.append(line)

            print(f"  ðŸ§ª [DEBUG] relevant_ids({len(relevant_ids)}): {relevant_ids}")

            id_to_paper = {p["arxiv_id"]: p for p in papers}
            sample_ids = list(id_to_paper.keys())[:10]
            print(f"  ðŸ§ª [DEBUG] papers arxiv_id æ ·ä¾‹({len(sample_ids)}): {sample_ids}")
            result = [id_to_paper[aid] for aid in relevant_ids if aid in id_to_paper]
            if result:
                return result[:top_k]
        except Exception as e:
            print(f"  âš ï¸  GPT ç­›é€‰å¤±è´¥: {e}")

        # GPT å¤±è´¥æ—¶é™çº§åˆ°å…³é”®è¯é¢„ç­›
        print("  âš ï¸  é™çº§åˆ°å…³é”®è¯é¢„ç­›é€‰")
        return self._keyword_prefilter(papers, top_k)

    def _keyword_prefilter(self, papers: List[Dict],
                           top_k: int = 10) -> List[Dict]:
        """
        å…³é”®è¯é¢„ç­›é€‰ï¼ˆä¸éœ€è¦ LLMï¼Œ0 å»¶è¿Ÿï¼‰

        ç”¨ bonus_keywords åšæ–‡æœ¬åŒ¹é…ï¼ŒæŒ‰å‘½ä¸­æ•°æŽ’åº
        """
        keywords = self.settings.bonus_keywords
        if not keywords:
            return papers[:top_k]

        scored = []
        for p in papers:
            text = (p.get("title", "") + " " + p.get("summary", "")).lower()
            hits = sum(1 for kw in keywords if kw.lower() in text)
            scored.append((hits, p))

        scored.sort(key=lambda x: x[0], reverse=True)
        return [p for _, p in scored[:top_k]]

    # ------------------------------------------------------------------
    # é™çº§æ‘˜è¦
    # ------------------------------------------------------------------

    def _fallback_summaries(self, papers: List[Dict]) -> Dict[str, str]:
        """LLM ä¸å¯ç”¨æ—¶çš„è§„åˆ™æ‘˜è¦ï¼ˆç”¨å…³é”®å¥æŠ½å–ç”Ÿæˆç®€ç‰ˆæ‘˜è¦ï¼‰"""
        results = {}
        for p in papers:
            abstract = p.get("summary", "")
            if not abstract:
                continue
            key = extract_key_sentences(abstract, max_sentences=3)
            # æˆªå–å‰ 200 å­—ç¬¦ï¼Œæ ¼å¼åŒ–ä¸ºè¦ç‚¹
            if len(key) > 200:
                key = key[:197] + "..."
            results[p["arxiv_id"]] = f"â€¢ {key}"
        return results

    # ------------------------------------------------------------------
    # æŠ¥å‘Šç”Ÿæˆ
    # ------------------------------------------------------------------

    def generate_report(self, result: Dict) -> str:
        papers = result.get("relevant", [])
        summaries = result.get("summaries", {})

        report = []
        report.append("=" * 80)
        report.append(f"arXiv æ™ºèƒ½æ—¥æŠ¥ - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}")
        report.append("=" * 80)
        report.append("")

        if not papers:
            report.append("ä»Šæ—¥æ— ç‰¹åˆ«ç›¸å…³çš„è®ºæ–‡ã€‚")
            return "\n".join(report)

        report.append(f"ðŸ“Š ä»Šæ—¥å…±å‘çŽ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡")
        report.append("")

        for i, paper in enumerate(papers, 1):
            score = paper.get("quality_score", 0)
            report.append(f"## {i}. {paper['title']}")
            report.append(f"**è´¨é‡è¯„åˆ†**: {score}/100")
            report.append(f"**arXiv ID**: {paper['arxiv_id']}")

            s2_authors = paper.get("s2_authors", [])
            valid_s2_authors = [a for a in s2_authors if (a.get("name") or "").strip()]
            if valid_s2_authors:
                parts = []
                for a in valid_s2_authors[:5]:
                    name = a.get("name", "")
                    affs = ", ".join(a.get("affiliations", []))
                    parts.append(f"{name} ({affs})" if affs else name)
                if len(valid_s2_authors) > 5:
                    parts.append("...")
                report.append(f"**ä½œè€…**: {'; '.join(parts)}")
            else:
                authors_str = ", ".join(paper.get("authors", [])[:3])
                if len(paper.get("authors", [])) > 3:
                    authors_str += "..."
                report.append(f"**ä½œè€…**: {authors_str}")

            report.append(f"**åˆ†ç±»**: {', '.join(paper.get('categories', []))}")

            citations = paper.get("s2_citation_count", 0)
            influential = paper.get("s2_influential_citation_count", 0)
            report.append(f"**å¼•ç”¨**: {citations} (æœ‰å½±å“åŠ›: {influential})")

            venue = paper.get("s2_venue", "")
            if paper.get("cr_published"):
                journal = paper.get("cr_journal", "") or venue
                doi = paper.get("cr_doi", "")
                pub_info = f"âœ… å·²å‘è¡¨ â€” {journal}"
                if doi:
                    pub_info += f" (DOI: {doi})"
                report.append(f"**å‘è¡¨çŠ¶æ€**: {pub_info}")
            elif venue:
                report.append(f"**å‘è¡¨çŠ¶æ€**: ðŸ“‹ {venue}")
            else:
                report.append(f"**å‘è¡¨çŠ¶æ€**: ðŸ“ é¢„å°æœ¬")

            report.append(f"**é“¾æŽ¥**: https://arxiv.org/abs/{paper['arxiv_id']}")
            report.append("")

            if paper["arxiv_id"] in summaries:
                report.append("**æ™ºèƒ½æ‘˜è¦**:")
                report.append(summaries[paper["arxiv_id"]])
            else:
                report.append("**åŽŸæ–‡æ‘˜è¦**:")
                report.append(paper["summary"][:300] + "...")

            report.append("")
            report.append("-" * 80)
            report.append("")

        return "\n".join(report)

    def _print_ranking(self, papers: List[Dict]):
        print("-" * 70)
        for i, p in enumerate(papers[:10], 1):
            citations = p.get("s2_citation_count", 0)
            venue = p.get("s2_venue", "") or p.get("cr_journal", "") or "â€”"
            status = "ðŸ“„" if p.get("cr_published") else "ðŸ“"
            score = p.get("quality_score", 0)
            print(f"  {i:>2}. [{score:>5.1f}åˆ†] {status} å¼•ç”¨:{citations:>4} "
                  f"| {venue[:20]:20s} | {p['title'][:45]}")
        print("-" * 70)
        print()

    def close(self):
        self.db.close()
