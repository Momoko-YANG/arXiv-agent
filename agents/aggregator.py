"""
Paper Aggregator â€” èšåˆæ‰€æœ‰æ•°æ®æº + è¯„åˆ† + ç­›é€‰ + æ‘˜è¦

è¿™æ˜¯ Agent ç³»ç»Ÿçš„æ ¸å¿ƒç¼–æŽ’å±‚ï¼š
    arXiv â†’ GPT ç­›é€‰ â†’ Semantic Scholar â†’ Crossref â†’ è¯„åˆ†æŽ’åº â†’ ä¸‰æ®µå¼æ‘˜è¦ â†’ å…¥åº“
"""

from datetime import datetime
from typing import List, Dict, Optional

from agents.arxiv_agent import ArxivAgent
from agents.semantic_agent import SemanticScholarClient
from agents.crossref_agent import CrossrefClient
from scoring import (
    ScoringPipeline, CitationScorer, AuthorScorer,
    VenueScorer, FreshnessScorer, KeywordScorer,
)
from summarizer.llm_summarizer import PaperSummarizer
from summarizer.prompt_templates import FILTER_SYSTEM, FILTER_PROMPT
from utils.llm_client import OpenAIClient
from utils.database import ArxivDatabase


class PaperAggregator:
    """
    è®ºæ–‡èšåˆ + åˆ†æž Agent

    ç”¨æ³•:
        agg = PaperAggregator(settings)
        result = agg.run_pipeline()
        report = agg.generate_report(result)
    """

    def __init__(self, settings):
        """
        Args:
            settings: config.settings.Settings å®žä¾‹
        """
        self.settings = settings

        # å„æ•°æ®æº Agent
        self.arxiv = ArxivAgent(categories=settings.categories)
        self.s2 = SemanticScholarClient(api_key=settings.s2_api_key)
        self.cr = CrossrefClient(mailto=settings.crossref_mailto)

        # LLM
        self.llm = OpenAIClient(
            api_key=settings.openai_api_key,
            model=settings.openai_model,
        )

        # è¯„åˆ†æµæ°´çº¿ï¼ˆå¯æ’æ‹”ï¼‰
        self.scorer = ScoringPipeline([
            CitationScorer(weight=30),
            AuthorScorer(weight=20),
            VenueScorer(weight=20),
            FreshnessScorer(weight=15),
            KeywordScorer(keywords=settings.bonus_keywords, weight=15),
        ])

        # ä¸‰æ®µå¼æ‘˜è¦
        self.summarizer = PaperSummarizer(llm_client=self.llm, language="zh")

        # æ•°æ®åº“
        self.db = ArxivDatabase(db_path=settings.db_path)

    # ------------------------------------------------------------------
    # å®Œæ•´æµæ°´çº¿
    # ------------------------------------------------------------------

    def run_pipeline(self) -> Dict:
        """
        å®Œæ•´å…­æ­¥æµæ°´çº¿

        Returns:
            {
                "papers":    æ‰€æœ‰è®ºæ–‡,
                "relevant":  Top N è®ºæ–‡,
                "summaries": {arxiv_id: summary},
            }
        """
        s = self.settings
        print(f"\n{'=' * 80}")
        print(f"å¼€å§‹æ™ºèƒ½æŠ“å–ä¸Žåˆ†æž - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'=' * 80}\n")

        # Step 1: arXiv
        print("ðŸ“¥ Step 1/6: ä»Ž arXiv æŠ“å–è®ºæ–‡...")
        papers = self.arxiv.fetch_recent_papers(days=s.days, max_results=s.max_results)
        if not papers:
            print("âŒ æ²¡æœ‰æŠ“å–åˆ°è®ºæ–‡")
            return {"papers": [], "relevant": [], "summaries": {}}
        print(f"  âœ… å…± {len(papers)} ç¯‡\n")

        # Step 2: GPT ç­›é€‰
        relevant = papers
        if s.research_interests:
            print("ðŸ¤– Step 2/6: GPT æ™ºèƒ½ç­›é€‰...")
            relevant = self._filter_relevant(papers, s.research_interests)
            print(f"  âœ… ç­›é€‰å‡º {len(relevant)} ç¯‡ç›¸å…³è®ºæ–‡\n")
        else:
            print("â© Step 2/6: è·³è¿‡ï¼ˆæœªè®¾ç½®ç ”ç©¶å…´è¶£ï¼‰\n")

        # Step 3: Semantic Scholar
        if relevant:
            print("ðŸ“¡ Step 3/6: Semantic Scholar è¡¥å……...")
            self.s2.enrich_papers(relevant)
            print()

        # Step 4: Crossref
        if relevant:
            print("ðŸ“– Step 4/6: Crossref å‘è¡¨çŠ¶æ€...")
            self.cr.enrich_papers(relevant)
            print()

        # è¯„åˆ†æŽ’åº
        if relevant:
            print("ðŸ“Š è¯„åˆ†æŽ’åº...")
            self.scorer.rank_papers(relevant)
            self._print_ranking(relevant)

        # æˆªå– Top N
        top_papers = relevant[:s.top_n]

        # Step 5: ä¸‰æ®µå¼æ‘˜è¦
        summaries = {}
        if top_papers:
            n = len(top_papers)
            print(f"ðŸ§  Step 5/6: ä¸‰æ®µå¼æ‘˜è¦ï¼ˆ{n} ç¯‡ï¼‰")
            print(f"   å…³é”®å¥æŠ½å– â†’ ç»“æž„åŒ–æå– â†’ è¯­ä¹‰åŽ‹ç¼©é‡å†™")
            summaries = self.summarizer.summarize_batch(top_papers, delay=1.0)
            print(f"  âœ… ç”Ÿæˆ {len(summaries)}/{n} ç¯‡æ‘˜è¦\n")

        # Step 6: å…¥åº“
        print("ðŸ’¾ Step 6/6: ä¿å­˜åˆ°æ•°æ®åº“...")
        new_count = sum(1 for p in papers if self.db.insert_paper(p))
        print(f"  âœ… æ–°å¢ž {new_count} ç¯‡\n")

        return {
            "papers": papers,
            "relevant": top_papers,
            "summaries": summaries,
        }

    # ------------------------------------------------------------------
    # GPT ç­›é€‰
    # ------------------------------------------------------------------

    def _filter_relevant(self, papers: List[Dict],
                         research_interests: str,
                         top_k: int = 10) -> List[Dict]:
        """ä½¿ç”¨ GPT ç­›é€‰æœ€ç›¸å…³çš„è®ºæ–‡"""
        papers_text = ""
        for i, p in enumerate(papers):
            papers_text += (
                f"{i+1}. ID: {p['arxiv_id']}\n"
                f"   æ ‡é¢˜: {p['title']}\n"
                f"   æ‘˜è¦: {p['summary'][:300]}...\n\n"
            )

        prompt = FILTER_PROMPT.format(
            research_interests=research_interests,
            papers_text=papers_text,
            top_k=top_k,
        )

        try:
            response = self.llm.chat(prompt, system=FILTER_SYSTEM)
            relevant_ids = []
            for line in response.strip().split("\n"):
                line = line.strip()
                if len(line) >= 10 and "." in line:
                    relevant_ids.append(line)

            id_to_paper = {p["arxiv_id"]: p for p in papers}
            return [id_to_paper[aid] for aid in relevant_ids if aid in id_to_paper][:top_k]

        except Exception as e:
            print(f"  âš ï¸  GPT ç­›é€‰å¤±è´¥: {e}")
            return papers[:top_k]

    # ------------------------------------------------------------------
    # æŠ¥å‘Šç”Ÿæˆ
    # ------------------------------------------------------------------

    def generate_report(self, result: Dict) -> str:
        """ç”Ÿæˆ Markdown æ—¥æŠ¥"""
        papers = result.get("relevant", [])
        summaries = result.get("summaries", {})

        report = []
        report.append("=" * 80)
        report.append(f"arXiv æ™ºèƒ½æ—¥æŠ¥ - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}")
        report.append("=" * 80)
        report.append("")

        if not papers:
            report.append("ä»Šæ—¥æ— ç‰¹åˆ«ç›¸å…³çš„è®ºæ–‡ã€‚")
            return "\n".join(report)

        report.append(f"ðŸ“Š ä»Šæ—¥å…±å‘çŽ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡")
        report.append("")

        for i, paper in enumerate(papers, 1):
            score = paper.get("quality_score", 0)
            report.append(f"## {i}. {paper['title']}")
            report.append(f"**è´¨é‡è¯„åˆ†**: {score}/100")
            report.append(f"**arXiv ID**: {paper['arxiv_id']}")

            # ä½œè€…ï¼ˆä¼˜å…ˆ S2 æœºæž„ä¿¡æ¯ï¼‰
            s2_authors = paper.get("s2_authors", [])
            if s2_authors:
                parts = []
                for a in s2_authors[:5]:
                    name = a.get("name", "")
                    affs = ", ".join(a.get("affiliations", []))
                    parts.append(f"{name} ({affs})" if affs else name)
                if len(s2_authors) > 5:
                    parts.append("...")
                report.append(f"**ä½œè€…**: {'; '.join(parts)}")
            else:
                authors_str = ", ".join(paper.get("authors", [])[:3])
                if len(paper.get("authors", [])) > 3:
                    authors_str += "..."
                report.append(f"**ä½œè€…**: {authors_str}")

            report.append(f"**åˆ†ç±»**: {', '.join(paper.get('categories', []))}")

            # å¼•ç”¨
            citations = paper.get("s2_citation_count", 0)
            influential = paper.get("s2_influential_citation_count", 0)
            report.append(f"**å¼•ç”¨**: {citations} (æœ‰å½±å“åŠ›: {influential})")

            # å‘è¡¨çŠ¶æ€
            venue = paper.get("s2_venue", "")
            if paper.get("cr_published"):
                journal = paper.get("cr_journal", "") or venue
                doi = paper.get("cr_doi", "")
                pub_info = f"âœ… å·²å‘è¡¨ â€” {journal}"
                if doi:
                    pub_info += f" (DOI: {doi})"
                report.append(f"**å‘è¡¨çŠ¶æ€**: {pub_info}")
            elif venue:
                report.append(f"**å‘è¡¨çŠ¶æ€**: ðŸ“‹ {venue}")
            else:
                report.append(f"**å‘è¡¨çŠ¶æ€**: ðŸ“ é¢„å°æœ¬")

            report.append(f"**é“¾æŽ¥**: https://arxiv.org/abs/{paper['arxiv_id']}")
            report.append("")

            # æ‘˜è¦
            if paper["arxiv_id"] in summaries:
                report.append("**æ™ºèƒ½æ‘˜è¦**:")
                report.append(summaries[paper["arxiv_id"]])
            else:
                report.append("**åŽŸæ–‡æ‘˜è¦**:")
                report.append(paper["summary"][:300] + "...")

            report.append("")
            report.append("-" * 80)
            report.append("")

        return "\n".join(report)

    # ------------------------------------------------------------------
    # è¾…åŠ©
    # ------------------------------------------------------------------

    def _print_ranking(self, papers: List[Dict]):
        """æ‰“å°è¯„åˆ†æŽ’å"""
        print("-" * 70)
        for i, p in enumerate(papers[:10], 1):
            citations = p.get("s2_citation_count", 0)
            venue = p.get("s2_venue", "") or p.get("cr_journal", "") or "â€”"
            status = "ðŸ“„" if p.get("cr_published") else "ðŸ“"
            score = p.get("quality_score", 0)
            print(f"  {i:>2}. [{score:>5.1f}åˆ†] {status} å¼•ç”¨:{citations:>4} "
                  f"| {venue[:20]:20s} | {p['title'][:45]}")
        print("-" * 70)
        print()

    def close(self):
        self.db.close()
